\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{colortbl}
\modulolinenumbers[5]

\journal{CIRP Journal of Manufacturing Science and Technology}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{A Spatial AR System for Wide-area Axis-aligned Augmentation of Planar Scenes in Industrial Settings} %\title{Elsevier \LaTeX\ template\tnoteref{mytitlenote}}
%\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Michael Horn\'{a}\v{c}ek}
\author{Hans K\"{u}ffner-McCauley}
\author{Sebastian Schlund}
\address{Human Centered Cyber Physical Production and Assembly Systems, Institute for Management Sciences, TU Vienna, Austria}

\begin{abstract}
Augmented reality (AR) promises to enable use cases in industrial settings that include the embedding of work steps directly into the scene, potentially reducing or altogether obviating the need for workers to refer to instructions in paper form or on a screen. In turn, spatial AR is a form of AR whereby the augmentation of the scene is carried out using a projector, with the advantage of rendering the augmentation visible to all onlookers simultaneously without calling for each to wear AR glasses. However, care must be taken to distort the images to be projected in a manner that they appear undistorted to the viewer, since the geometry of the scene as it relates to the geometry of the projector plays a role in how the pixels of the projector's image plane map to points in the scene. For planar scene geometry (such as a floor, wall, or table), this can be done in a cumbersome manual process called keystone correction, often using software bundled with the projector.

We propose a system that produces the effect of keystone correction analytically, intuitively placing the desired augmentations in a manner aligned with the axes of an image of the scene acquired by a camera. Moreover, our system is able to handle a projector equipped with a steerable mirror, enabling wide-area factory floor augmentation.
\end{abstract}

\begin{keyword}
Spatial augmented reality (SAR) \sep Industry 4.0 \sep Pilotfabrik
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}\label{sec:intro}

Augmented reality (AR) promises to enable use cases in industrial settings that include the embedding of work steps directly into the scene, potentially reducing or altogether doing away with the need for workers to refer to instructions in paper form or on a screen. Typically, AR works by embedding the augmentation in an image of the scene acquired from the viewpoint of a single individual, with the resulting augmented image in turn displayed using some form of AR glasses. Reliance on AR glasses has two adverse consequences: (i) AR glasses must be worn by each individual wishing to view the augmentation, and (ii) such glasses---in some cases taking the form of a helmet in order to house multiple sensors in support of accurately tracking the viewpoint of the viewer relative to the scene---can be obtrusive. Spatial AR is a form of augmented reality carried out not by embedding the augmentation in an image of the scene as with AR glasses, but by projection to the scene itself, thus eliminating both aforementioned problems. Yet considering for the moment a planar surface to be augmented, unless the projector faces the surface frontally, the bounds of a projected rectangular image will not appear rectangular; more generally, they will instead appear trapezoidal (i.e., the image will appear distorted). Such distortions can be eliminated by carrying out a cumbersome manual process called keystone correction, often using software bundled with the projector.

Using the $X$- and $Y$-axes of an image of the scene as a proxy, our contribution is to propose a system that produces the effect of keystone correction analytically, and in a manner aligning the axes of the augmentation with those of the proxy image. We achieve this by distorting the image to be projected using a plane-induced homography computed to produce the effect of projecting the image not from the actual projector viewpoint, but in accordance with the viewpoint of a \textit{virtual} projector (i) facing directly downwards to the scene plane and (ii) rotated to place the axes of the image plane of the virtual projector in line with those of the camera. This enables intuitive placement of augmentations in the scene, and eliminates the need for manual keystone correction. Moreover, our system is able to handle a projector equipped with a steerable mirror without need for explicitly modeling the action of the steerable mirror on the projector.



\subsection{Related Work}

\section{Hardware Setup}

The hardware setup employed in this work comprised a Panasonic XXX projector with a steerable mirror system manufactured by XX. In addition, we used a Zed 2 stereo camera manufactured by Stereolabs, yet relied only on the left view. The setup was mounted on the ceiling of the Pilotfabrik, a collaborative space for research on Industry 4.0 topics situated in Vienna, Austria. The floorspace used for our experiments measured dimensions of X~m~$\times$~Y~m; the projector was mounted at approximately the center of this space, at a height of ca.\ XX m. 

\section{Approach}


The problem of correcting for distortions as outlined in Section~\ref{sec:intro} requires knowledge of the manner in which the respective rays through the pixels of the projector's image plane fan out into the scene, and the geometry of the scene itself within at least the projectorâ€™s field of view, as illustrated in Figure 1. This is because the scene point `illuminated' by a pixel in the projector's image plane is given by intersecting its corresponding ray with the geometry of the scene surface. To model this interaction calls for a one-time camera-projector calibration, which includes recovery of the scene plane. Next, the relative camera-projector-scene plane geometry is used to compute a plane-induced homography.

The hardware setup employed 

\subsection{Camera-projector Calibration}

\paragraph{Camera 2D-3D correspondences}

\paragraph{Projector 2D-3D correspondences}

\subsection{Distortion}

\paragraph{Virtual projector}

\paragraph{Plane-induced homography} Let $\mathtt{K}_\text{proj}$ express the $3~\times~3$ calibration matrix of the projector and $(\mathtt{R}', \mathbf{t}') \in SE(3)$ the rigid body transformation that transforms points from the coordinate frame of the projector to that of the virtual projector.

\begin{equation}
\mathtt{H} = \mathtt{K}_\text{proj}\left(\mathtt{R}' + \frac{1}{Z^{} \mathbf{n}^\top \mathbf{p}^{}}\mathbf{t}'\mathbf{n}^\top\right)\mathtt{K}_\text{proj}^{-1},
\label{homgen}
\end{equation}
where ()

\section{Evaluation}

\section{Conclusion}

Here are two sample references: \cite{Feynman1963118,Dirac1953888}.

\bibliography{mybibfile}

\end{document}